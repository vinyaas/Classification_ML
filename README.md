## Machine Learning Classification Projects

Welcome to my machine learning classification projects repository! This repository contains various classification projects where I experiment with different algorithms and evaluate their performance using a range of metrics.

## Project Overview
In this repository, I explore machine learning classification techniques such as logistic regression, K-nearest neighbors (KNN), decision trees, random forest and Naive Bayes. These projects demonstrate my skills in coding, data analysis, and machine learning. The dataset and project specifics are explained in each subproject folder.

## Key Features
Here are some key features and methodologies utilized in this repository:

- **Data Cleaning**: Techniques for cleaning and preprocessing raw data, including handling missing values, encoding categorical variables, and scaling numerical features.

- **Exploratory Data Analysis (EDA)**: Insightful exploration of the dataset's characteristics, distributions, correlations, and potential relationships between variables.

- **Feature Selection**: Methods to identify the most relevant features for classification modeling, enhancing model interpretability and performance.

- **Model Development**: Various classification techniques, including logistic regression, K-nearest neighbors, decision trees, and Naive Bayes, to build predictive models for categorical target variables.

The following Python packages are used in this repository:

- [numpy](https://numpy.org/doc/)
- [pandas](https://pandas.pydata.org/docs/)
- [matplotlib](https://matplotlib.org/stable/contents.html)
- [seaborn](https://seaborn.pydata.org/tutorial.html)
- [scikit-learn](https://scikit-learn.org/stable/user_guide.html)

  ## Machine Learning Models
The machine learning models explored in this repository include:

- **Logistic Regression**: A linear approach to binary classification.
- **K-Nearest Neighbors (KNN)**: A non-parametric method based on proximity to neighbors.
- **Decision Trees**: Tree-based structure for classification with splits based on features.
- **Naive Bayes**: A probabilistic classifier based on Bayes' theorem, assuming feature independence.

## Performance Metrics
To evaluate these models, I use several metrics, including:

- **Accuracy**: Proportion of correct predictions out of total predictions.
- **Precision**: Proportion of true positives among predicted positives.
- **Recall**: Proportion of true positives among actual positives.
- **F1-Score**: Harmonic mean of precision and recall.

## Results
The results of each project are detailed in their respective Jupyter Notebooks, where you'll find visualizations, performance metrics, and discussions on model behavior.

